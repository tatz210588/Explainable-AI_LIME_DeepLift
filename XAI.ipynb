{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NClPD46swLvD"
      },
      "source": [
        "Author: Tathagat Saha (Matricola no: 902046)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-DHqaP6uP5r"
      },
      "source": [
        "## Explaining Recommender System\n",
        "\n",
        "Till now we have built a recommender system using the Neural Matrix Factorization framework. This framework allowed us to combine the GMF layers with the MLP layers.\n",
        "\n",
        "Let's focus on the MLP model:\n",
        "<center>  <img src=\"https://drive.google.com/uc?export=view&id=1rL_8kkHIhSlQjWr8hNal4Tyog87-2kNP\" width=\"550\" height=\"350\"> </center> \n",
        "\n",
        "It easy to understand that the prediction is not computed over the user or the item indeces per se, but ove the embedding learned and produced. So, it could be useful to understand how these embeddings are produced and used to make the prediction. \n",
        "\n",
        "Let's try to apply some additive feature attribution method to such embeddings.\n",
        "\n",
        "- TASK 1: Apply LIME to the MLP model by keeping the embedding layers out of the forward function and using the embedding produced as input to the model. This can be done without modifying the forward function and using the predefined methods from the class [captum.attr.InterpretableEmbeddingBase](https://captum.ai/api/utilities.html#).\n",
        "\n",
        "- TASK 2: Apply LIME to the NeuMF model and check the results.\n",
        "\n",
        "- TASK 3 [OPTIONAL]: choose another additive feature attribution method and apply it to the MLP model, following the implementation you prefere from Task 1.\n",
        "\n",
        "\n",
        "Suggestion: the best way to work on embedding is that of using  the InterpretableEmbeddingBase class. In particular, you can use the  configure_interpretable_embedding_layer method to create interpretation over the embedding layer. Pay attention to input and output dimensions of the surrogate model [This tutorial](https://captum.ai/tutorials/Multimodal_VQA_Interpret) may help you in better understanding this concept and, particularly, how to use additive feature models with embeddings. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlBy_GK3ZdN0",
        "outputId": "e479475b-f625-4d04-80c8-fb2bea9076ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: captum in /usr/local/lib/python3.9/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from captum) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from captum) (3.7.1)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.9/dist-packages (from captum) (2.0.0+cu118)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->captum) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->captum) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->captum) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->captum) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->captum) (3.11.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->captum) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6->captum) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6->captum) (16.0.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (5.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (1.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (23.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->captum) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6->captum) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#import pandas_profiling as pdp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from os import path\n",
        "import os\n",
        "\n",
        "#scikit-learn related imports\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# pytorch relates imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "!pip install captum\n",
        "\n",
        "# imports from captum library\n",
        "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
        "from captum.attr import DeepLift, KernelShap, DeepLiftShap, ShapleyValueSampling\n",
        "\n",
        "from captum.attr import Lime, LimeBase\n",
        "from captum._utils.models.linear_model import SkLearnLinearRegression, SkLearnLasso\n",
        "from captum.attr._core.lime import get_exp_kernel_similarity_function\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from captum.attr import (\n",
        "    IntegratedGradients,\n",
        "    LayerIntegratedGradients,\n",
        "    TokenReferenceBase,\n",
        "    configure_interpretable_embedding_layer,\n",
        "    remove_interpretable_embedding_layer,\n",
        "    visualization\n",
        ")\n",
        "from captum.attr._utils.input_layer_wrapper import ModelInputWrapper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30LBCS8SuaGB",
        "outputId": "f06a1a50-548a-43e7-aa76-acc8521c08b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "num_sample_data = '100k'\n",
        "\n",
        "MODEL_PATH_MLP = 'drive/MyDrive/Colab Notebooks/movielens_{}/MLP.pt'.format(num_sample_data) #change this with your directory \n",
        "MODEL_PATH_NeuMF = 'drive/MyDrive/Colab Notebooks/movielens_{}/neuMF.pt'.format(num_sample_data) #change this with your directory \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX7FX99oswx5"
      },
      "source": [
        "First we collect a sample data for applying LIME to different models. And then select any single value to apply."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "07uYDq1Kpq6d"
      },
      "outputs": [],
      "source": [
        "input_user_indices = torch.tensor([124,  53,  41, 336, 706, 781, 794,  19,  23, 617,  11, 545, 421,  58,\n",
        "        823, 453, 897, 587, 573,  37, 477, 239, 531, 570,  65,  19, 496,  22,\n",
        "        739, 537, 602,  19,  26, 792,  70, 782, 837, 892, 837, 496, 598, 311,\n",
        "        421, 388, 489, 865, 728, 402, 739,  38, 605, 883, 318,  65, 674,   3,\n",
        "          0, 409,  92, 504, 263, 759,  45,   3, 797, 752, 757, 158, 749,  87,\n",
        "        814, 278,  68,  85, 915, 317, 617, 365, 402, 701,  80, 694, 136, 751,\n",
        "        505, 136, 839, 162, 321, 841, 217, 549, 817,  30, 933, 556, 202, 720,\n",
        "        450, 845, 703, 796, 784, 616, 428,  91,  35,   4, 409, 939, 617, 517,\n",
        "        264,  37, 536,   6, 928, 409,  99, 601, 117, 791, 402, 530, 660, 432,\n",
        "        621, 648,  41, 390, 244, 205, 279, 196,  41, 797, 532, 531, 715, 627,\n",
        "        499,  39, 177, 129,  37,  26,  35, 857, 477,  17, 195, 369, 117, 167,\n",
        "        413, 428, 635,  12, 345, 752, 409,  58, 819, 137, 442, 366, 369, 455,\n",
        "        202, 778,  37, 862,  65, 118, 793, 366,  58, 817, 625, 742, 778, 940,\n",
        "        893, 602, 131, 887,   8, 376, 862, 532, 934, 467, 628,   3, 703, 329,\n",
        "         16, 103, 862, 394,  63, 423, 747, 565, 229, 381, 421, 803,  96, 867,\n",
        "        310, 864, 384, 786, 543, 695, 315, 145, 464, 166, 542, 430, 742, 223,\n",
        "        745, 311,  58,  93, 655, 878, 912, 325, 689, 352, 491, 195,  23, 441,\n",
        "        234, 899,  48, 873, 391,  90,  19, 476, 802, 162,  38,  20, 685, 180,\n",
        "        600, 881, 925, 179])\n",
        "input_label_indices = torch.tensor([1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
        "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
        "        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
        "        0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
        "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
        "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
        "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
        "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
        "        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
        "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
        "        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
        "        0., 0., 0., 1.])\n",
        "input_item_indices = torch.tensor([ 118,  405,  212,  819,   29,  222,  239, 1174,  238, 1443,  247,  725,\n",
        "          31,  645, 1651,  327, 1039, 1116, 1601,  752,  564,   91, 1437, 1419,\n",
        "        1348, 1484,   81,  300, 1446,  717, 1030, 1370, 1472,  198, 1584,   41,\n",
        "         867,   99,  582,  524,  596,  404,   15, 1517, 1157,  789,  625,  595,\n",
        "        1266,  802,   24, 1611,  851,  314, 1677,  532,  632,  149,  607,  774,\n",
        "        1563,  133,  910,  423,  976,  520, 1242, 1205, 1601,   85,  721,  473,\n",
        "         815,  155, 1215, 1387,   18,  428,  851,  373,  126,  283,  473,  649,\n",
        "        1218,   58,  743,  683, 1184, 1243,  462,  479,  606,  244,  696,  245,\n",
        "         441, 1624, 1339, 1142,    5, 1473, 1419,  560, 1401,   28,  589,   51,\n",
        "        1662,  372, 1207, 1627,  907, 1597,  229,   47, 1146, 1160,  231, 1303,\n",
        "        1432,  824,  852, 1318,  569,  267,  129,  967,  392,  456,  479,  862,\n",
        "           7,  120, 1190,  623,  319,  867, 1405,  724,  375,   31,  919,  270,\n",
        "        1477,  562,  548, 1401, 1262,  928,   79,  940, 1423,  269,  746, 1177,\n",
        "        1568,  773,  266, 1537, 1420,  855,  577,   14,  437,  217,  927,  203,\n",
        "         668,   42,   95, 1274,  469,  422, 1143, 1474, 1487, 1444,  423,  114,\n",
        "          48,  802,  621, 1537, 1496, 1600, 1366,  344,  228,  455, 1126,  166,\n",
        "         928, 1162, 1218,   30,   94, 1322, 1335,  198, 1044,  551, 1174, 1351,\n",
        "         614,  367, 1171,  782,  408,  958,  762,  787, 1464, 1157,  579,   78,\n",
        "        1041,  102,  240,  679, 1146, 1063, 1294, 1258,  430,  727,  593,   35,\n",
        "        1318, 1302,  321,  625, 1178,  687,  293,  301,  127,  245,  723,  647,\n",
        "         747,  696,  577,  135,  581,  251, 1368,  330, 1337,  751, 1345,  805,\n",
        "        1543, 1227, 1356,  647])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6beuYjrZB1M",
        "outputId": "09c8d767-cb7f-44ab-c292-6448c185fdaf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/captum/attr/_models/base.py:191: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lime attribute for MLP are (tensor([[ 0.0121, -0.0208,  0.0584,  0.0350, -0.2202,  0.0256,  0.0184, -0.0500,\n",
            "          0.0247, -0.0678, -0.0172,  0.0729, -0.0996,  0.0368, -0.1170, -0.0071,\n",
            "         -0.1568, -0.0013, -0.0082,  0.0357, -0.0222, -0.0128, -0.0091, -0.0671,\n",
            "         -0.1252, -0.0062,  0.0631,  0.0326, -0.0715,  0.0159, -0.1495, -0.0395]]), tensor([[-1.4448e-04, -3.7393e-01, -2.2949e-02,  5.7151e-02, -1.3964e-02,\n",
            "          1.3760e-01,  4.1288e-02,  1.7292e-02, -9.3369e-02, -4.4838e-02,\n",
            "         -7.4124e-02,  9.5524e-02,  2.5529e-02, -2.1355e-02,  5.1708e-02,\n",
            "         -7.5598e-02,  5.3712e-02,  7.7821e-02,  6.6502e-02, -1.8127e-01,\n",
            "          4.6816e-02,  1.1386e-02, -2.3367e-02,  4.8847e-02, -2.8622e-02,\n",
            "          9.1473e-03,  6.1751e-02,  5.6433e-02, -5.2631e-02, -3.7514e-03,\n",
            "          1.0204e-01,  2.1102e-02]]))\n",
            "Prediction for 6th value using neuMF model is 0\n",
            "Label for 6th value using MLP model is 0.0\n"
          ]
        }
      ],
      "source": [
        "# Task 1\n",
        "modelMLP = torch.load(MODEL_PATH_MLP, map_location=device) #load the pre-trained Model\n",
        "netMLP = torch.load(MODEL_PATH_MLP, map_location=device) #load the pre-trained Model\n",
        "\n",
        "idx = 6 #select any id for choosing a data\n",
        "\n",
        "interpretable_emb_user = configure_interpretable_embedding_layer(netMLP,'embedding_user')\n",
        "interpretable_emb_item = configure_interpretable_embedding_layer(netMLP,'embedding_item')\n",
        "\n",
        "prediction = modelMLP(input_user_indices[idx],input_item_indices[idx]) #Get the prediction of the model on the given single data\n",
        "\n",
        "input_emb_user = interpretable_emb_user.indices_to_embeddings(input_user_indices[idx])\n",
        "input_emb_item = interpretable_emb_item.indices_to_embeddings(input_item_indices[idx])\n",
        "\n",
        "exp_eucl_distance = get_exp_kernel_similarity_function('euclidean', kernel_width=1000)\n",
        "\n",
        "lr_lime = Lime(\n",
        "    netMLP, \n",
        "    interpretable_model=SkLearnLinearRegression(),  # build-in wrapped sklearn Linear Regression\n",
        "    similarity_func=exp_eucl_distance\n",
        ")\n",
        "\n",
        "attr = lr_lime.attribute((input_emb_user.view(1,-1),input_emb_item.view(1,-1)), n_samples=100)\n",
        "print('Lime attribute for MLP are {}'.format(attr))\n",
        "print('Prediction for {}th value using MLP model is {}'.format(idx,1)) if prediction > 0.5 else print('Prediction for {}th value using neuMF model is {}'.format(idx,0))\n",
        "print('Label for {}th value using MLP model is {}'.format(idx,input_label_indices[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVyIMl5gwK0v",
        "outputId": "a4e1bf04-4144-4d83-cab5-afa2d2812b1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lime attribute for neuMF are (tensor([[ 0.0051, -0.0712, -0.0636, -0.0483, -0.1804, -0.0693,  0.0402,  0.0150,\n",
            "         -0.1247,  0.0132,  0.0080, -0.0691, -0.0964,  0.0695, -0.0262,  0.0786,\n",
            "         -0.0064,  0.0093, -0.0740, -0.0750, -0.0338,  0.0796,  0.0151,  0.0542,\n",
            "         -0.1026,  0.0574, -0.0561,  0.0243,  0.0033, -0.0300, -0.0014, -0.0038]]), tensor([[-0.0200, -0.0278,  0.0148, -0.0180,  0.0165, -0.1284, -0.0075,  0.0214,\n",
            "          0.0538,  0.0788,  0.0133,  0.0465, -0.0357,  0.0338,  0.1059, -0.0530,\n",
            "         -0.0529, -0.0487, -0.0101, -0.0925,  0.0904, -0.0194,  0.0444,  0.0067,\n",
            "          0.0339, -0.0849,  0.0310, -0.1009,  0.0801, -0.0679, -0.0188,  0.0069]]))\n",
            "Prediction for 90th value using neuMF model is 0\n",
            "Actual Label for 90th value using neuMF model is 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/captum/attr/_models/base.py:191: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Task 2\n",
        "modelneuMF = torch.load(MODEL_PATH_NeuMF, map_location=device) #load the pre-trained Model\n",
        "netneuMF = torch.load(MODEL_PATH_NeuMF, map_location=device) #load the pre-trained Model\n",
        "\n",
        "idx = 90 #select any id for choosing a data\n",
        "\n",
        "interpretable_emb_user1 = configure_interpretable_embedding_layer(netneuMF,'embedding_user_GMF')\n",
        "interpretable_emb_item1 = configure_interpretable_embedding_layer(netneuMF,'embedding_item_GMF')\n",
        "\n",
        "prediction = modelneuMF(input_user_indices[idx],input_item_indices[idx]) #Get the prediction of the model on the given single data\n",
        "\n",
        "input_emb_user1 = interpretable_emb_user1.indices_to_embeddings(input_user_indices[idx])\n",
        "input_emb_item1 = interpretable_emb_item1.indices_to_embeddings(input_item_indices[idx])\n",
        "\n",
        "exp_eucl_distance = get_exp_kernel_similarity_function('euclidean', kernel_width=1000)\n",
        "\n",
        "lr_lime = Lime(\n",
        "    netneuMF, \n",
        "    interpretable_model=SkLearnLinearRegression(),  # build-in wrapped sklearn Linear Regression\n",
        "    similarity_func=exp_eucl_distance\n",
        ")\n",
        "\n",
        "attr = lr_lime.attribute((input_emb_user1.view(1,-1),input_emb_item1.view(1,-1)), n_samples=100)\n",
        "remove_interpretable_embedding_layer(netneuMF, interpretable_emb_user1)\n",
        "remove_interpretable_embedding_layer(netneuMF, interpretable_emb_item1)\n",
        "print('Lime attribute for neuMF are {}'.format(attr))\n",
        "print('Prediction for {}th value using neuMF model is {}'.format(idx,1)) if prediction > 0.5 else print('Prediction for {}th value using neuMF model is {}'.format(idx,0))\n",
        "print('Actual Label for {}th value using neuMF model is {}'.format(idx,input_label_indices[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7fTP9rtFmih",
        "outputId": "345d0538-0084-43b3-b82d-1be001b75524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DeepLift attribute for MLP model are (tensor([[-2.1367e-04,  8.0887e-03,  1.0362e-01,  2.2262e-02, -1.7562e-01,\n",
            "         -6.9118e-04, -5.8024e-03, -5.2917e-03,  1.5857e-02, -1.2039e-01,\n",
            "          1.4820e-03, -2.2773e-03, -1.4333e-01,  8.6720e-02, -1.2584e-01,\n",
            "         -1.0729e-02, -2.5563e-01, -1.3054e-02,  6.3040e-04, -4.2994e-03,\n",
            "         -1.3471e-03, -6.5906e-03, -4.6718e-02, -1.5959e-02, -1.3743e-01,\n",
            "         -2.2429e-02,  9.4424e-02,  1.2648e-02, -7.0995e-02, -2.0196e-02,\n",
            "         -7.1867e-02, -1.8032e-02]], grad_fn=<MulBackward0>), tensor([[ 5.8646e-02, -2.8480e-01, -2.4862e-02,  2.4976e-02, -5.3873e-03,\n",
            "          7.4753e-02, -2.1247e-04,  2.2422e-03, -4.3256e-02, -3.6812e-02,\n",
            "          3.8539e-03,  5.8308e-02,  7.5212e-03, -2.1494e-02, -2.0720e-04,\n",
            "         -2.0040e-02,  5.1701e-03,  8.4333e-02,  3.8598e-03, -4.9603e-02,\n",
            "          1.3901e-02, -1.6433e-04,  5.1911e-03,  9.6673e-03,  1.2261e-02,\n",
            "          9.9057e-03, -4.0715e-03,  5.8596e-02, -2.6044e-02,  3.3372e-02,\n",
            "          2.7125e-02,  1.4611e-02]], grad_fn=<MulBackward0>))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
            "               activations. The hooks and attributes will be removed\n",
            "            after the attribution is finished\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Task 3\n",
        "# As an Additive Feature Attribution Method with CAPTUM we are using DeepLift algorith using the embeddings already made in Task 1.\n",
        "# Warning: Please run it only and only after executing Task 1\n",
        "\n",
        "dl = DeepLift(netMLP)\n",
        "dl_attr_test = dl.attribute((input_emb_user.view(1,-1),input_emb_item.view(1,-1)))\n",
        "print('DeepLift attribute for MLP model are {}'.format(dl_attr_test))\n",
        "remove_interpretable_embedding_layer(netMLP, interpretable_emb_user)\n",
        "remove_interpretable_embedding_layer(netMLP, interpretable_emb_item)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.9 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9-final"
    },
    "vscode": {
      "interpreter": {
        "hash": "0355315a2ab493fe43adfbd0e3e2d1269f7b92070986f6468c200dc91ca7ae56"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
